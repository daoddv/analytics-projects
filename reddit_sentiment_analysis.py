# -*- coding: utf-8 -*-
"""BUS5PR1_Assignment02_Sprint02_DucDao_Reddit.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pxUaSM778-1GZASCo2t9W91RSsbsxoBN

# Electric Vehicle Communication Analytics   
Sprint 01: NLP Exploration  
Prepared by EV Communication Solutions team  
Duc Dao - 20427752  
Huyen Pham - 19802570

*BUS5PR1 - Analytics Project 1   
Assignment 02: The Hustle - EV Analytics*

In this report we will analyze the data from comment on Twitter, Whirlpool, Reddit, Ozbargain and propose communication strategies.  
- At this Sprint 01, we will perform NLP Exploration on the given dataset.

# Load Reddit dataset
"""

# import library
import pandas as pd

# load reddit tweet dataset
df_reddit = pd.read_excel("EV_community_discussion_data.xlsx", sheet_name='Reddit')

# View the dataset
df_reddit

"""Select a subset of columns required for the analysis"""

df_reddit = df_reddit[['date', 'comment']]

print('total number of records in the dataset:', df_reddit.shape[0])

# check missing values
df_reddit.isnull().sum()

# check the data type to make sure date is in the right format
df_reddit.dtypes

"""# Data Pre-processing

**Count the number of words in each comment**
"""

#@title
df_reddit['word_count'] = df_reddit['comment'].apply(lambda x: len(str(x).split(" ")))
df_reddit.sort_values('word_count', ascending=[False])[:5]

"""**Count the number of characters of each comment**"""

df_reddit['char_count'] = df_reddit['comment'].str.len()  # Includes the spaces
df_reddit.sort_values('char_count', ascending=[False])[:5]

"""**Box plot analysis for number of words and characters in each comment**

There are some comments contain lot of words may need to remove because according to the box plot, most of the tweets have less than ~320 words  
And most of the comments have less than 2500 characters, thus, need to remove some of them
"""

df_reddit[['word_count']].boxplot()

df_reddit[['char_count']].boxplot()

"""### **Remove extreme values**  
Remove any comment that has more than 320 words and 2500 characters
"""

df_reddit = df_reddit[(df_reddit["word_count"] < 320) & (df_reddit["char_count"] < 2500)]
df_reddit.info

# return the selected columns for next steps
df_reddit = df_reddit[['date','comment']]

"""# Text pre-processing

### Transform to lowercase
Because Python is a case-sensitive language. Thus case distinction would be unnecessary
"""

df_reddit['comment'] = df_reddit['comment'].str.lower()
# View the df
df_reddit.head()

"""### Remove duplicate values"""

# Remove duplicate values
duplicate_count = len(df_reddit['comment'])-len(df_reddit['comment'].drop_duplicates())
print('duplicate count:', duplicate_count)
print('total records before remove duplicates:', df_reddit.shape[0])

# drop duplicates (keep the last comment of each of the duplicates)
df_reddit = df_reddit.drop_duplicates(subset='comment', keep="first")
print('updated record count:', df_reddit.shape[0])

"""### Remove special characters
The given dataset contains some urls and unique characters (@, #) which are not relevant for analysis, thus those words will be removed
"""

# Import Regular Expression Python module
import re

# Define function
def cleanUpComment(comment):
  # remove any sequence of characters followed by '@' sign
  unique_tweet = re.sub(r"(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)", "", str(comment))
  
  # remove multiple spaces
  spaces_removed = re.sub(r"\s\s+", " ", str(unique_tweet)) 

  return spaces_removed

df_reddit['comment'] = df_reddit['comment'].apply(cleanUpComment)
df_reddit.head(5)

"""### Remove regular expressions
A regular expression is a special sequence of characters that helps to match or find other strings or sets of strings. Does not help the machine learning process.
"""

df_reddit['comment'] = df_reddit['comment'].str.replace('[^\w\s]','')
df_reddit.head(5)

"""### Remove digits
In this sentiment analysis numbers or digits are not relevant. Therefore, remove any digit in the Tweet
"""

# define the function
def remove_digits(sent):
  return " ".join(w for w in sent.split() if not w.isdigit())

df_reddit['comment'] = df_reddit['comment'].apply(remove_digits)
df_reddit.head()

"""### Remove Stopwords
Stopwords are irrelevant for NLP purposes because they occur frequently in the language. Therefore, they need to be removed from the dataset.  
"""

# Load NLTK library
import nltk

# Download the stopwords to the nltk library
nltk.download('stopwords')

# Load the stopwords
from nltk.corpus import stopwords

# create stop words list
stop = stopwords.words('english')

# Remove not because 'not' is negative impact on sentiment analysis
stop.remove('not')

# Add some words to stop word list
sw_list = ['cant', 'dont', 'doesnt', 'didnt','even', 'get', 'got', 'im','isnt', 'one', 'thats', 'theyre', 'youre', 'theres', 'people', 'would', 'could', 'still', 'many', 'much', 'also', 'every' , 'well', ]
stop.extend(sw_list)

# Print the list of stop words
print(stop)

# Remove stop words (from NLTK stop word list) from the Tweets
df_reddit['comment'] = df_reddit['comment'].apply(lambda x: " ".join(x for x in x.split() if x not in stop))
df_reddit.head(3)

"""### Standardization of comment  
**Stemming**  
Remove prefix, suffix etc, to derive the base form of a word
"""

# import library
from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

# define the function
def stemming_function(sent):
  word_list = sent.split()
  stemmed_word_list = [stemmer.stem(word) for word in word_list]
  stemmed_sentence = " ".join(stemmed_word_list)
  return stemmed_sentence

df_reddit['comment_stemmed'] = df_reddit['comment'].apply(stemming_function)
df_reddit.head()

"""**Lemmatization**  
Converting the given word into it's base form according to the dictionary meaning of the word.
"""

# Download wordnet
nltk.download('wordnet')

# import library
from nltk.stem import WordNetLemmatizer
lemmtizer = WordNetLemmatizer()

# Define the function
def lemmatize_function(sent):
  word_list = sent.split()
  lemma_word_list = [lemmtizer.lemmatize(word) for word in word_list]
  lemma_sentence = " ".join(lemma_word_list)
  return lemma_sentence

df_reddit['comment_lemmatized'] = df_reddit['comment'].apply(lemmatize_function)
df_reddit

"""There are a lot of wording issues in Stemming, ex: remov, littl, electr,.. Thought observation, Lemmatization algorithm seems to be working better in this case when compared to the Stemmming algorith.  
**Therefore, we will user Lemmatized data for further analysis**
"""

#df_reddit['comment'] = df_reddit['comment'].apply(lemmatize_function)
df_reddit = df_reddit[['date','comment']]

"""### Word frequency analysis"""

word_frequency = pd.Series(' '.join(df_reddit['comment']).split()).value_counts()

# Top common words
word_frequency[:10]  # get top 10

# import library
import matplotlib.pyplot as plt
import seaborn as sns

# Visualize the top word counts 
word_count  = word_frequency
word_count = word_count[:10,]
plt.figure(figsize=(10,5))
sns.barplot(word_count.index, word_count.values, alpha=0.8)
plt.title('Comment in top 10 words')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('Word', fontsize=12)
plt.show()

# import library
from PIL import Image
from wordcloud import WordCloud

# create word cloud
corpus = list(df_reddit['comment'])

wordcloud = WordCloud(background_color='white', max_words=200, max_font_size=50, random_state=42).generate(str(corpus))

fig = plt.figure(1)
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

"""From the word cloud we could identify that the terms **car, not, electric, ev** has been mentioned frequently. Some of the words are expected - it will not generate much an insight.  
**Therefore, those high frequency non-insightful words will be removed.**

### Remove common words  
Based on the context, those expected and non-insightful words will be removed from the dataset
"""

# Creating a list of custom stopwords
new_words_to_remove = ["car", 'cars', "electric", "ev",'evs', 'vehicle', 'vehicles', 'australia', 'not', 'like', 'think', 'need', 'going', 'really']

# Remove common words using lambda function
df_reddit['comment'] = df_reddit['comment'].apply(lambda x: " ".join(x for x in x.split() if x not in new_words_to_remove))
df_reddit.head(5)

"""## Re-attempt frequency word list"""

# Create a word frequency series. (This is a pandas series)
word_frequency = pd.Series(' '.join(df_reddit['comment']).split()).value_counts()

# Look at the top 10 words
word_frequency[:10]

# Visualize the top word counts 
word_count  = word_frequency
word_count = word_count[:10,]
plt.figure(figsize=(10,5))
sns.barplot(word_count.index, word_count.values, alpha=0.8)
plt.title('Comment in top 10 words')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('Word', fontsize=12)
plt.show()

# Generate word cloud
corpus = list(df_reddit['comment'])

wordcloud = WordCloud(background_color='white', max_words=200, max_font_size=50, random_state=42).generate(str(corpus))

fig = plt.figure(1)
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

"""# N-grams analysis
An **n-gram** is a contiguous sequence of n items from a given sample of text or speech. They are basically a set of co-occuring words within a given window. When computing the n-grams, the shift is one-step forward (although you can move X words forward in more advanced scenarios)
"""

# import library for N-grams
from sklearn.feature_extraction.text import CountVectorizer

from nltk.tokenize import word_tokenize
from nltk.util import ngrams
nltk.download('punkt')

# Define function to generate most frequently occuring Bi-grams
def get_ngrams(corpus, ngram_range=(2, 2)):
    
    # Create CountVectorizer object from sklearn library with bigrams
    vec1 = CountVectorizer(ngram_range=ngram_range, max_features=2000).fit(corpus)

    # Create BoW feature representation using word frequency
    bag_of_words = vec1.transform(corpus)

    # compute sum of words
    sum_words = bag_of_words.sum(axis=0) 

    # create (word, frequency) tuples for bigrams
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec1.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq

"""## **Bigrams**"""

bigrams_comment = get_ngrams(df_reddit['comment'].tolist(), ngram_range=(2, 2))

# Convert bigrams of both datasets to a dataframe with column names bi-gram and frequency.
bigrams_comment_df = pd.DataFrame(bigrams_comment)
bigrams_comment_df.columns=["Bi-gram", "Freq"]

# top 10 
bigrams_comment_df.head(10)

# Barplot of most freq Bi-grams
top_bigrams_to_show = 20

sns.set(rc={'figure.figsize':(13,8)})
h=sns.barplot(x="Bi-gram", y="Freq", data=bigrams_comment_df[:top_bigrams_to_show])
h.set_xticklabels(h.get_xticklabels(), rotation=60)  # here rotation parameter shows the angle of your x-axis labels
plt.title('Comment - Bigram Analysis')
plt.show()

"""**insights**

## **Trigrams**
"""

trigrams_comment = get_ngrams(df_reddit['comment'].tolist(), ngram_range=(3, 3))

# Convert trigrams of both datasets to a dataframe with column names tri-gram and frequency.
trigrams_comment_df = pd.DataFrame(trigrams_comment)
trigrams_comment_df.columns=["Tri-gram", "Freq"]

# top 10
trigrams_comment_df.head(10)

# Barplot of most freq Tri-grams
top_trigrams_to_show = 20

sns.set(rc={'figure.figsize':(13,8)})
h=sns.barplot(x="Tri-gram", y="Freq", data=trigrams_comment_df[:top_trigrams_to_show])
h.set_xticklabels(h.get_xticklabels(), rotation=60)
plt.title('Comment - Trigram Analysis')
plt.show()

"""**insight**

# Comment stats

## Post performance
"""

df_reddit_stats = pd.read_excel("EV_community_discussion_data.xlsx", sheet_name='Reddit')

len(df_reddit_stats['title'].unique())

# count nbr of comment by title
df_reddit_stats_title_comment_count = df_reddit_stats.groupby('title', as_index = False)[['comment']].count().sort_values(['comment'],ascending=False)
df_reddit_stats_title_comment_count.head(30)

"""## Post engagement"""

# count nbr of comment by day
df_reddit_stats_day_comment_count = df_reddit_stats.groupby('date', as_index = False)[['comment']].count().sort_values(['comment'],ascending=False)
df_reddit_stats_day_comment_count.head(5)

# visualization
df_reddit_stats_day_comment_count = df_reddit.groupby(['date'])['comment'].count().reset_index(name='comment_count').set_index('date')

ax = df_reddit_stats_day_comment_count.plot()
plt.savefig('temporal_analysis.png', dpi=800)

# Plot comment sentiment timeline
import plotly.express as px
df_reddit_stats_day_comment_count = df_reddit.groupby(['date'])['comment'].count().reset_index(name='comment_count')

fig = px.line(df_reddit_stats_day_comment_count, x="date", y="comment_count", title='Comment over time on Reddit')
fig.show()

"""# Sentiment Analysis  

Using Sentiment analysis to determining the attitude or the emotion of the writer, i.e., whether it is positive or negative or neutral.  

*Using Textblob library. The sentiment function of textblob returns the polarity of the sentence, i.e., a float value which lies in the range of [-1,1] where 1 means positive statement and -1 means a negative statement and 0 mean neutral.*
"""

# import library
from textblob import TextBlob

"""Derive sentiment of each sentence."""

# Extract sentiments from the comments
df_reddit['sentiment'] = df_reddit['comment'].apply(lambda x: TextBlob(x).sentiment.polarity)

# View first 20 comments with respective sentiment value
print(df_reddit[['comment', 'sentiment']][:20])

"""**Categorize of comments**  as Negative, Neutral and Positive."""

# define function negative, nautral, positive analysis
def getTextAnalysis(a):
    if a < 0:
        return "Negative"
    elif a == 0:
        return "Neutral"
    else:
        return "Positive"

# apply function and create another feature called Score and display the first 20 values.
df_reddit['Score'] = df_reddit['sentiment'].apply(getTextAnalysis)
df_reddit[['comment', 'sentiment', 'Score']].head(20)

"""## Overall sentiment score

Calculate the percentage of positive comments from all the tweets
"""

positive = df_reddit[df_reddit['Score'] == 'Positive']
print(str(positive.shape[0]/(df_reddit.shape[0])*100) + " % of positive comments")

negative = df_reddit[df_reddit['Score'] == 'Negative']
print(str(negative.shape[0]/(df_reddit.shape[0])*100) + " % of negative comments")

neutral = df_reddit[df_reddit['Score'] == 'Neutral']
print(str(neutral.shape[0]/(df_reddit.shape[0])*100) + " % of neutral comments")

"""**Representation of Sentiments of Words**"""

import matplotlib.pyplot as plt

labels = df_reddit.groupby('Score').count().index.values
values = df_reddit.groupby('Score').size().values
colors = ['orange', 'lightblue', 'green']

plt.bar(labels, values, color = colors)
plt.xlabel("Sentiments of words")
plt.ylabel("Count of Sentiment")



"""We can see how Negative, Neutral and Positive tweets about climate change since early 2018

## Generate word cloud  
By positive & negative sentiments

**Commonly used positive words**
"""

corpus = list(df_reddit[(df_reddit['Score'] == 'Positive')]['comment'])

wordcloud = WordCloud(background_color='white', max_words=200, max_font_size=50, random_state=42).generate(str(corpus))

fig = plt.figure(1)
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

word_frequency = pd.Series(' '.join(df_reddit[(df_reddit['Score'] == 'Positive')]['comment']).split()).value_counts()
word_frequency[:10]

# Visualize the top common positive word counts 
word_count  = word_frequency
word_count = word_count[:10,]
plt.figure(figsize=(10,5))
sns.barplot(word_count.index, word_count.values, alpha=0.8)
plt.title('Commonly used positive words')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('Word', fontsize=12)
plt.show()

"""**Commonly used negative words**"""

corpus = list(df_reddit[(df_reddit['Score'] == 'Negative')]['comment'])

wordcloud = WordCloud(background_color='white', max_words=200, max_font_size=50, random_state=42).generate(str(corpus))

fig = plt.figure(1)
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

word_frequency = pd.Series(' '.join(df_reddit[(df_reddit['Score'] == 'Negative')]['comment']).split()).value_counts()
word_frequency[:10]

# Visualize the top common positive word counts 
word_count  = word_frequency
word_count = word_count[:10,]
plt.figure(figsize=(10,5))
sns.barplot(word_count.index, word_count.values, alpha=0.8)
plt.title('Commonly used negative words')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('Word', fontsize=12)
plt.show()

"""## Sentiment over time  
Analyse sentiments for each comment, aggregate into the sentiment of particular day and derive a sentiment timeline.

Plot sentiment over time.
"""

# Plot comment sentiment timeline
# df_senti_reddit_date_wise = df_reddit.groupby(['date'])['sentiment'].mean().reset_index(name='mean_sentiment_comment').set_index('date')
# ax = df_senti_reddit_date_wise.plot()

df_reddit_avg_senti = df_reddit.groupby(['date'])['sentiment'].mean().reset_index(name='mean_sentiment')

# Plot comment sentiment timeline
import plotly.express as px

fig = px.line(df_reddit_avg_senti, x="date", y="mean_sentiment", title='Sentiment over time on Reddit')
fig.show()

"""**let's see the comments and titles from 20 to 23 Feb**"""

# comments and titles on 20 to 23 Feb
# df_reddit_stats[(df_reddit_stats['date'] >= '2020-02-20') & (df_reddit_stats['date'] <= '2020-02-23')]
df_reddit_stats[(df_reddit_stats['date'] == '2020-02-17') | (df_reddit_stats['date'] == '2020-02-23')]

"""# Topic modeling"""

# create new df for modeling
df_processed = df_reddit

# import libraries
import tempfile
import logging

# Setting up the environment for LDA algorithm.
temp_df = tempfile.gettempdir()
print('Folder "{}" will be used to save temporary dictionary and corpus.'.format(temp_df))
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

# Convert the tweets as the text corpus.
corpus = list(df_processed['comment'])

# view first 5 copus
corpus[:5]

# Tokenization
df_texts = [[word for word in str(document).split()] for document in corpus]

# display first 5 values
df_texts[:5]

# import libraries
from gensim import corpora, models, similarities
import os

# Create a dictionary based on the tokanized words of all the tweets.
dictionary = corpora.Dictionary(df_texts)

# Save the above dictionary as a local file for LDA model to access.
dictionary.save(os.path.join(temp_df, 'df.dict'))

# Print the dictionary
print(dictionary.token2id)

# Convert the text dictionary to bag of words model
corpus = [dictionary.doc2bow(text) for text in df_texts]

# convert tweet to tokens and present it as id from the dictionary
comment_id = 0
print(df_texts[comment_id]) # each tweet converted to tokens
print(dictionary.doc2bow(df_texts[comment_id])) # each token is represented as a id from a dictionary

"""## Generate the topic model"""

# Construct TF-IDF features from the dictionary.
tfidf = models.TfidfModel(corpus)

# Transform the tweets as TF-IDF feature vectors
corpus_tfidf = tfidf[corpus]

#Define 20 topics capture through LDA.
total_topics = 20

#Build the LDA topic model.
lda = models.LdaModel(corpus, id2word=dictionary, num_topics=total_topics)
corpus_lda = lda[corpus_tfidf]

# Print the Keyword in the 10 topics
lda.show_topics(total_topics, num_words=6)

"""**insight**
From the keywords of the topics above we can see the importance of a keyword to the topic.  

**For example**: for the first topic: '0.065*"usa" + 0.035*"greenhousegas" + 0.035*"day" + 0.026*"india" + 0.024*"canada" + 0.022*"ghgt14"'). We can see the importance of the countries (usa, india, canada) and greenhouse gas and day in this topic. The weight of usa in this topic os 0.065. This topic could be the comparison of greenhouse gas emission between three countries over time.  

**The discussion in Australia on global warming and sustainability** has result: '0.093*"melbourne" + 0.084*"sydney" + 0.076*"sustainability" + 0.058*"brisbane" + 0.052*"canberra" + 0.050*"globalwarming"'). In which, state (location) names' weight are 0.093, 0.084, 0.058 and 0.052 for melbourne, sydney, brisbane and canberra.

## Interactive topic analyzer
"""

!pip install pyLDAvis==2.1.2
import pyLDAvis.gensim

pyLDAvis.enable_notebook()
panel = pyLDAvis.gensim.prepare(lda, corpus_lda, dictionary, mds='tsne')
panel